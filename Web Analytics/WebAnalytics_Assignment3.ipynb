{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Analytics HW3\n",
    "## TwitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from TwitterAPI import TwitterAPI\n",
    "import pandas as pd\n",
    "# initialize TwitterAPI and necessary keys and tokens\n",
    "CONSUMER_KEY = 'fzrSWX4oRD0OEOFnKQYDGfPSD' \n",
    "CONSUMER_SECRET = 'vV9gU8RT8cLNnufsg36CVYl7Qy40WlcHOgtbza3Chlxd48yTZp'\n",
    "ACCESS_TOKEN_KEY = '891155597344813056-11oAOCuIQxNDhR7sWhb42th35h30XHr'\n",
    "ACCESS_TOKEN_SECRET = 'u3SH0UvczuBxgPas5P1ghWivJm7hVsq2eZV5hhKnCwpCu'\n",
    "\n",
    "api = TwitterAPI(\n",
    "    CONSUMER_KEY,\n",
    "    CONSUMER_SECRET,\n",
    "    ACCESS_TOKEN_KEY,\n",
    "    ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_function(company_list):\n",
    "    count = 100\n",
    "    lang = 'en'\n",
    "    id_list = []\n",
    "    for i in company_list:\n",
    "        search_term = i\n",
    "        r = api.request('search/tweets',{'q':search_term,'count':count,'lang':lang})\n",
    "        with open (i+'.txt','a') as file1:\n",
    "            for item in r:\n",
    "                try:\n",
    "                    if item['id'] in id_list:\n",
    "                        pass\n",
    "                    # in order to save to csv.file, I exclued tweets which with '\\r' or '\\n' \n",
    "                    # because pandas.to_csv will mistake these data\n",
    "                    elif '\\n' in str(item['text']) or '\\r' in str(item['text']):\n",
    "                        pass\n",
    "                    else:\n",
    "                        id_list.append(item['id'])\n",
    "                        file1.write(str(item['user']['screen_name'])+'::'+str(item['user']['location'])+'::'+str(item['text'])+'::'+str(item['retweet_count'])+'\\n')\n",
    "\n",
    "                except UnicodeEncodeError:\n",
    "                        pass\n",
    "def txt2csv(company_list):\n",
    "    crawled_df = pd.DataFrame(columns = ['username','location','tweet_text','count_of_retweets','company'])\n",
    "    for i in company_list:\n",
    "        temp = pd.read_csv((i+'.txt'),sep = '::',header = None, engine = 'python')\n",
    "        temp.loc[:,'company'] = i\n",
    "        temp.columns = ['username','location','tweet_text','count_of_retweets','company']\n",
    "        crawled_df = pd.concat([crawled_df,temp])\n",
    "    return crawled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "companies = ['Amazon','Costco','ebay']\n",
    "# run crawl_function 2 times\n",
    "crawl_function(companies)\n",
    "# convert txt to csv\n",
    "df = txt2csv(companies)\n",
    "# save to csv file\n",
    "df.to_csv('C:\\Users\\youch\\Desktop\\crawled_byTwitterAPI.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tweepy\n",
    "### extra credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_tweepy(company_list):\n",
    "    \"\"\"use tweepy cursor to crawl tweet data\n",
    "       clean \\r and \\n in order to save to csv file\"\"\"\n",
    "    # initialize tweepy api\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN_KEY,ACCESS_TOKEN_SECRET)\n",
    "    api = tweepy.API(auth)\n",
    "    for i in company_list:\n",
    "        with open (i+'.txt','a') as file2:\n",
    "            for tweet in tweepy.Cursor(api.search,q=i,result_type=\"recent\",lang=\"en\").items(100):\n",
    "                if '\\n' in str(tweet.text.encode('UTF-8')) or '\\r' in str(tweet.text.encode('UTF-8')):\n",
    "                    pass\n",
    "                else:\n",
    "                    name = tweet.user.screen_name.encode('UTF-8')\n",
    "                    location = tweet.user.location.encode('UTF-8')\n",
    "                    tweet_content = tweet.text.encode('UTF-8')\n",
    "                    retweet = tweet.retweet_count\n",
    "                    file2.write(str(name)+'::'+str(location)+'::'+str(tweet_content)+'::'+str(retweet)+'\\n')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "companies = ['Amazon','Costco','ebay']\n",
    "# run two times\n",
    "crawl_tweepy(companies)\n",
    "# convert and save to csv file\n",
    "df_tweepy = txt2csv(companies)\n",
    "df_tweepy.to_csv('C:\\Users\\youch\\Desktop\\crawled_byTweepy.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
