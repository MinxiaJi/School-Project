{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmx\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "#  Question 1 #\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 1 partition data\n",
    "# load option data name as 'option'\n",
    "option = pd.read_csv('Option_Data_2000.csv')\n",
    "option.head()\n",
    "# train80% test20%\n",
    "x_train,x_test,y_train,y_test = train_test_split(option.drop('Implied Volatility',axis = 1),\n",
    "                                                 option['Implied Volatility'],test_size = 0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "#  Question 2 #\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "#   sklearn KNN   #\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Models under sklearn\n",
    "# build k-nn classifier(clf) model\n",
    "# prevent valueError : unknown label error\n",
    "clf = KNeighborsRegressor()\n",
    "clf.fit(x_train,y_train)\n",
    "# save predictions to KNNpreds\n",
    "KNNpreds = clf.predict(x_test)\n",
    "# convert KNNpreds to series data\n",
    "KNNpreds = pd.DataFrame(KNNpreds)\n",
    "\n",
    "# add actual IV, implied volatility to GBpreds\n",
    "# to form a dataframe with column one:IV column two:predict IV\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "KNNcompare = pd.concat([KNNpreds,y_test],axis = 1)\n",
    "# To array and convert to RDD\n",
    "KNNRDD = KNNcompare.as_matrix()\n",
    "KNNRDD = sc.parallelize(KNNRDD)\n",
    "\n",
    "# x[0]:actual IV, x[1] predict IV\n",
    "KNNError = KNNRDD.map(lambda x:abs(x[0]-x[1]))\n",
    "# get x_test index as RDD\n",
    "indexRDD = sc.parallelize(x_test.index)\n",
    "# combine indexRDD with ErrorRDD\n",
    "indexedKNNError = indexRDD.zip(KNNError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23, 0.051800000000000013),\n",
       " (29, 0.20848),\n",
       " (30, 0.10687999999999998),\n",
       " (32, 0.090700000000000003),\n",
       " (44, 0.083320000000000005),\n",
       " (45, 0.015079999999999982),\n",
       " (49, 0.080319999999999947),\n",
       " (56, 0.20100000000000001),\n",
       " (59, 0.14094000000000007),\n",
       " (63, 0.082519999999999982),\n",
       " (65, 0.055739999999999956),\n",
       " (67, 0.11635999999999999),\n",
       " (69, 0.0043600000000000305),\n",
       " (70, 0.014479999999999993),\n",
       " (73, 0.10450000000000001),\n",
       " (76, 0.034799999999999998),\n",
       " (78, 0.033599999999999963),\n",
       " (99, 0.089439999999999992),\n",
       " (100, 0.086499999999999994),\n",
       " (109, 0.030879999999999963),\n",
       " (111, 0.04226000000000002),\n",
       " (115, 0.050320000000000004),\n",
       " (120, 0.038739999999999997),\n",
       " (123, 0.34243999999999991),\n",
       " (124, 0.38330000000000003),\n",
       " (128, 0.015039999999999998),\n",
       " (135, 0.00084000000000000741),\n",
       " (162, 0.067840000000000011),\n",
       " (163, 0.088940000000000019),\n",
       " (168, 0.043240000000000056),\n",
       " (173, 0.023499999999999993),\n",
       " (175, 0.029420000000000002),\n",
       " (185, 0.045940000000000036),\n",
       " (188, 0.030100000000000016),\n",
       " (194, 0.0069400000000000017),\n",
       " (196, 0.05536000000000002),\n",
       " (203, 0.0030000000000000027),\n",
       " (210, 0.040179999999999993),\n",
       " (211, 0.0090800000000000325),\n",
       " (212, 0.0031800000000000161)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get samples of top 10% error\n",
    "reversedKNNError = indexedKNNError.takeOrdered(int(0.1*len(x_test)))\n",
    "reversedKNNError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1745, 0.00013999999999991797),\n",
       " (1100, 0.00017999999999998573),\n",
       " (792, 0.00023999999999996247),\n",
       " (1766, 0.00025999999999992696),\n",
       " (712, 0.00029999999999996696),\n",
       " (1987, 0.00061999999999995392),\n",
       " (1550, 0.00068000000000001393),\n",
       " (135, 0.00084000000000000741),\n",
       " (1190, 0.0008799999999999919),\n",
       " (1640, 0.0010999999999999899),\n",
       " (1487, 0.0011000000000000454),\n",
       " (433, 0.0011400000000000299),\n",
       " (1541, 0.0012599999999999278),\n",
       " (630, 0.0013000000000000234),\n",
       " (1897, 0.0014799999999999813),\n",
       " (610, 0.0017000000000000071),\n",
       " (1659, 0.0018400000000000083),\n",
       " (1767, 0.0019000000000000128),\n",
       " (1741, 0.0019799999999998708),\n",
       " (1310, 0.0022199999999999998),\n",
       " (411, 0.002260000000000012),\n",
       " (824, 0.0023400000000000087),\n",
       " (1164, 0.0023799999999999932),\n",
       " (787, 0.0024000000000000132),\n",
       " (1464, 0.0027400000000000202),\n",
       " (203, 0.0030000000000000027),\n",
       " (1782, 0.0031200000000000117),\n",
       " (1813, 0.0031200000000001227),\n",
       " (212, 0.0031800000000000161),\n",
       " (730, 0.0032000000000000084),\n",
       " (1225, 0.0034200000000000064),\n",
       " (1233, 0.0034399999999999986),\n",
       " (1518, 0.0035800000000000276),\n",
       " (1054, 0.0036199999999999566),\n",
       " (879, 0.0036799999999999888),\n",
       " (1103, 0.0036799999999999888),\n",
       " (298, 0.0039399999999999713),\n",
       " (1223, 0.0042000000000000093),\n",
       " (69, 0.0043600000000000305),\n",
       " (785, 0.004400000000000015)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get samples of bottom 10% error\n",
    "sortedKNNError = indexedKNNError.sortBy(lambda x:x[1])\n",
    "sortedKNNError.take(int(0.1*len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "#   sklearn GB    #\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build GB model\n",
    "params = {'n_estimators':1000,'max_depth':6,'min_samples_split':2,'learning_rate':0.01,'loss':'ls'}\n",
    "gb = GradientBoostingRegressor(**params)\n",
    "gb.fit(x_train,y_train)\n",
    "# save predictions to GBpreds\n",
    "GBpreds = gb.predict(x_test)\n",
    "\n",
    "# convert GBpreds to series data\n",
    "GBpreds = pd.DataFrame(GBpreds)\n",
    "# add actual IV, implied volatility to GBpreds\n",
    "# to form a dataframe with column one:IV column two:predict IV\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "GBcompare = pd.concat([GBpreds,y_test],axis = 1)\n",
    "# To array and convert to RDD\n",
    "GBRDD = GBcompare.as_matrix()\n",
    "GBRDD = sc.parallelize(GBRDD)\n",
    "\n",
    "# x[0]:actual IV, x[1] predict IV\n",
    "GBError = GBRDD.map(lambda x:abs(x[0]-x[1]))\n",
    "# get x_test index as RDD\n",
    "indexRDD = sc.parallelize(x_test.index)\n",
    "# combine indexRDD with ErrorRDD\n",
    "indexedGBError = indexRDD.zip(GBError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23, 0.063195153324744457),\n",
       " (29, 0.013855456163301527),\n",
       " (30, 0.045339279041976321),\n",
       " (32, 0.049258871675226523),\n",
       " (44, 0.079317050728843097),\n",
       " (45, 0.047240137812337657),\n",
       " (49, 0.041338561700472981),\n",
       " (56, 0.043022774051756918),\n",
       " (59, 0.12680052655083957),\n",
       " (63, 0.12397819818334893),\n",
       " (65, 0.003628423917705681),\n",
       " (67, 0.076498201045796049),\n",
       " (69, 0.031813598988998826),\n",
       " (70, 0.014660572443932707),\n",
       " (73, 0.15044956343513846),\n",
       " (76, 0.019225190949456766),\n",
       " (78, 0.076832890144398536),\n",
       " (99, 0.004785646704587615),\n",
       " (100, 0.0071576513425320454),\n",
       " (109, 0.041490808097703058),\n",
       " (111, 0.0080941459162026108),\n",
       " (115, 0.01501204930131339),\n",
       " (120, 0.011371412830498612),\n",
       " (123, 0.023481003756251562),\n",
       " (124, 0.065243867979530995),\n",
       " (128, 0.013513840497212548),\n",
       " (135, 0.043891923600209881),\n",
       " (162, 0.022032403635743436),\n",
       " (163, 0.039683358279559711),\n",
       " (168, 0.023303886004079011),\n",
       " (173, 0.0075163490563118496),\n",
       " (175, 0.0043491759871339408),\n",
       " (185, 0.0022348939888435226),\n",
       " (188, 0.034705286651028955),\n",
       " (194, 0.016231023567923442),\n",
       " (196, 0.016500910412592357),\n",
       " (203, 0.010556910647969098),\n",
       " (210, 0.066868189804986933),\n",
       " (211, 0.027813691029099863),\n",
       " (212, 0.0056846752605488882)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get samples of top 10% error\n",
    "reversedGBError = indexedGBError.takeOrdered(int(0.1*len(x_test)))\n",
    "reversedGBError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1754, 8.9803157614365414e-05),\n",
       " (1510, 9.9706183353420741e-05),\n",
       " (1233, 0.00021606157011888616),\n",
       " (993, 0.00041412438869675716),\n",
       " (1100, 0.00063626623256388126),\n",
       " (1245, 0.00067034201368390556),\n",
       " (307, 0.00068477237622988074),\n",
       " (1794, 0.00076118638304650821),\n",
       " (620, 0.00084116152344620998),\n",
       " (1756, 0.00091019684238563547),\n",
       " (534, 0.0010129386418914199),\n",
       " (888, 0.0011644412273803528),\n",
       " (1621, 0.0011906543387777968),\n",
       " (1242, 0.0012336416790506566),\n",
       " (1103, 0.0014711156835513217),\n",
       " (1563, 0.0014959494995072653),\n",
       " (572, 0.001675338209990801),\n",
       " (1911, 0.0019639491446526436),\n",
       " (788, 0.0019956505435512195),\n",
       " (535, 0.0020773115455924029),\n",
       " (1116, 0.0020798601873592837),\n",
       " (185, 0.0022348939888435226),\n",
       " (1818, 0.0024277785186761491),\n",
       " (591, 0.0024546559079760555),\n",
       " (422, 0.0026571332398348524),\n",
       " (1562, 0.0026909332683838771),\n",
       " (1105, 0.0034611106036376849),\n",
       " (65, 0.003628423917705681),\n",
       " (305, 0.0036845815328005738),\n",
       " (855, 0.0036874881779368207),\n",
       " (1173, 0.0037368284975521637),\n",
       " (905, 0.0039116120287224221),\n",
       " (1990, 0.0039920749430659708),\n",
       " (393, 0.0041518691373737815),\n",
       " (175, 0.0043491759871339408),\n",
       " (1673, 0.0044793260937986412),\n",
       " (1313, 0.00456949217829411),\n",
       " (530, 0.0047457889258427366),\n",
       " (99, 0.004785646704587615),\n",
       " (1491, 0.0048001237682759879)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get samples of bottom 10% error\n",
    "sortedGBError = indexedGBError.sortBy(lambda x:x[1])\n",
    "sortedGBError.take(int(0.1*len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0091367752425267172"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare MSE of gradientboosting and KNN\n",
    "GBMSE = GBRDD.map(lambda v: (v[0] - v[1])**2).sum() / float(y_test.count())\n",
    "KNNMSE = KNNRDD.map(lambda v: (v[0] - v[1])**2).sum() / float(y_test.count())\n",
    "GBMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012650478147000002"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNNMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "#  Question 4 #\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM and GB under Spark\n",
    "# train80% test20%\n",
    "trainData, testData = train_test_split(option,test_size=0.2,random_state=42)\n",
    "train = trainData.as_matrix()\n",
    "test = testData.as_matrix()\n",
    "def parsePoint(line):\n",
    "    return LabeledPoint(line[7],line[0:7])\n",
    "# create RDD\n",
    "trainRDD = sc.parallelize(train)\n",
    "testRDD = sc.parallelize(test)\n",
    "trainLP = trainRDD.map(parsePoint)\n",
    "testLP = testRDD.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build GB model\n",
    "GBmodel = GradientBoostedTrees.trainRegressor(trainLP,\n",
    "                                            categoricalFeaturesInfo={5:2}, numIterations=3)\n",
    "predictions = GBmodel.predict(testLP.map(lambda x: x.features))\n",
    "sparkGBError = testLP.map(lambda lp: lp.label).zip(predictions)\n",
    "# compute MSE\n",
    "testMSE = sparkGBError.map(lambda v: (v[0] - v[1])**2).sum() / float(testLP.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019821288888369395"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build SVM model\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o404.trainSVMModelWithSGD.\n: org.apache.spark.SparkException: Input validation failed.\r\n\tat org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:256)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:92)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainSVMModelWithSGD(PythonMLLibAPI.scala:248)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-dc65a2759803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclfSVM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVMWithSGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainLP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\opt\\spark\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\mllib\\classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, data, iterations, step, regParam, miniBatchFraction, initialWeights, regType, intercept, validateData, convergenceTol)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[1;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVMModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\opt\\spark\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\mllib\\regression.py\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[0;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_convert_to_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\opt\\spark\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\mllib\\classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rdd, i)\u001b[0m\n\u001b[1;32m    549\u001b[0m             return callMLlibFunc(\"trainSVMModelWithSGD\", rdd, int(iterations), float(step),\n\u001b[1;32m    550\u001b[0m                                  \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminiBatchFraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVMModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\opt\\spark\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\opt\\spark\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\opt\\spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\opt\\spark\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\opt\\spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o404.trainSVMModelWithSGD.\n: org.apache.spark.SparkException: Input validation failed.\r\n\tat org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:256)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:92)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainSVMModelWithSGD(PythonMLLibAPI.scala:248)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "clfSVM = SVMWithSGD.train(trainLP, iteration=100)\n",
    "svmLabelAndPreds = testLP.map(lambda p: (p.label, clfSVM.predict(p.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.4453, [111.26,121.0,7.0,0.0026,0.05,0.0,0.161134794]),\n",
       " LabeledPoint(0.2832, [19.0,19.0,35.0,0.0026,0.65,1.0,0.245222124]),\n",
       " LabeledPoint(0.3192, [44.03,35.0,553.0,0.0026,2.0,1.0,0.236108918]),\n",
       " LabeledPoint(0.2766, [44.03,43.0,63.0,0.0026,4.4,0.0,0.236108918]),\n",
       " LabeledPoint(0.8867, [7.17,11.0,35.0,0.0026,0.25,0.0,0.706962799])]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLP.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
